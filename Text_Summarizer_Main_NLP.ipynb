{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_bELa-Ks22F",
        "outputId": "ceffa4a0-f471-4188-c602-c2fef64fc5b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (0.1.97)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.11.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-niNeuFs4fc",
        "outputId": "a19bd3ad-6326-49a4-b19f-33487db1e2fa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import gutenberg as gut\n",
        "\n",
        "print(gut.fileids())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M3rhXxAtB7h",
        "outputId": "81a29a7d-c581-41ba-dc45-fceb9263b522"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "macbeth_text = nltk.corpus.gutenberg.raw('shakespeare-macbeth.txt')\n",
        "macbeth_text[:500]"
      ],
      "metadata": {
        "id": "dkQuwhD3tDGD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "380f86b8-7d9a-4611-b591-5b2589f9654f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[The Tragedie of Macbeth by William Shakespeare 1603]\\n\\n\\nActus Primus. Scoena Prima.\\n\\nThunder and Lightning. Enter three Witches.\\n\\n  1. When shall we three meet againe?\\nIn Thunder, Lightning, or in Raine?\\n  2. When the Hurley-burley's done,\\nWhen the Battaile's lost, and wonne\\n\\n   3. That will be ere the set of Sunne\\n\\n   1. Where the place?\\n  2. Vpon the Heath\\n\\n   3. There to meet with Macbeth\\n\\n   1. I come, Gray-Malkin\\n\\n   All. Padock calls anon: faire is foule, and foule is faire,\\nHouer through \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.utils import to_categorical\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence.lower()\n",
        "\n",
        "macbeth_text = preprocess_text(macbeth_text)\n",
        "text=macbeth_text[:450]"
      ],
      "metadata": {
        "id": "aNXQZIix_Rql"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#input_text = \"This is a sample input text. It contains multiple sentences. We want to summarize this text using BERT.\"\n",
        "sentences = text.split(\".\")\n",
        "summary = \". \".join(sentences[:2]) + \".\"\n"
      ],
      "metadata": {
        "id": "qIbf1OsAs4jj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertLMHeadModel\n",
        "\n",
        "model = BertLMHeadModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVcBEfulwzMd",
        "outputId": "ee504169-18b8-4ad4-f5da-faa7912115fd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "#summary_ids = model.generate(input_ids, num_beams=4, max_length=150, early_stopping=True)\n",
        "summary_ids = model.generate(input_ids, \n",
        "                              num_beams=4, \n",
        "                              max_length=150, \n",
        "                              temperature=1.0, \n",
        "                              do_sample=True,\n",
        "                              early_stopping=True,\n",
        "                              no_repeat_ngram_size=2,\n",
        "                              length_penalty=1.0,\n",
        "                              num_return_sequences=1)\n",
        "\n",
        "\n",
        "summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "DWpE28Ses4m_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy0E1Bg1I4-l",
        "outputId": "0cd29171-f7ad-4804-e9a7-9e8599b213bd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  101,  1996, 19817, 18655,  2666,  1997, 25182,  2011,  2520,  8101,\n",
            "          2552,  2271, 26927,  7606,  8040,  8913,  2532, 21111,  8505,  1998,\n",
            "          7407,  4607,  2093, 12566,  2043,  4618,  2057,  2093,  3113,  2153,\n",
            "          2063,  1999,  8505,  7407,  2030,  1999,  4542,  2063,  2043,  1996,\n",
            "         25124, 20934, 12866,  2589,  2043,  1996,  7151, 14162,  2063,  2439,\n",
            "          1998,  2180,  2638,  2008,  2097,  2022,  9413,  2063,  1996,  2275,\n",
            "          1997,  3103,  2638,  2073,  1996,  2173, 21210,  2239,  1996,  9895,\n",
            "          2045,  2000,  3113,  2007, 25182,  2272,  3897, 15451,  4939,  2035,\n",
            "         11687,  7432,  4455,  2019,  2239,  4189,  2063,  2003, 12487,  2063,\n",
            "          1998, 12487,  2063,  2003,  4189,  2063,  7570, 13094,  2083,  1996,\n",
            "          9666,  3351,  1998, 10882, 24658,  2666,  1037, 16363,  4654,   102]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(summary_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POxIPp6EI_16",
        "outputId": "842f9c1d-6b75-4bdc-dc15-68b9ba2f2561"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  101,  1996, 19817, 18655,  2666,  1997, 25182,  2011,  2520,  8101,\n",
            "          2552,  2271, 26927,  7606,  8040,  8913,  2532, 21111,  8505,  1998,\n",
            "          7407,  4607,  2093, 12566,  2043,  4618,  2057,  2093,  3113,  2153,\n",
            "          2063,  1999,  8505,  7407,  2030,  1999,  4542,  2063,  2043,  1996,\n",
            "         25124, 20934, 12866,  2589,  2043,  1996,  7151, 14162,  2063,  2439,\n",
            "          1998,  2180,  2638,  2008,  2097,  2022,  9413,  2063,  1996,  2275,\n",
            "          1997,  3103,  2638,  2073,  1996,  2173, 21210,  2239,  1996,  9895,\n",
            "          2045,  2000,  3113,  2007, 25182,  2272,  3897, 15451,  4939,  2035,\n",
            "         11687,  7432,  4455,  2019,  2239,  4189,  2063,  2003, 12487,  2063,\n",
            "          1998, 12487,  2063,  2003,  4189,  2063,  7570, 13094,  2083,  1996,\n",
            "          9666,  3351,  1998, 10882, 24658,  2666,  1037, 16363,  4654,   102,\n",
            "          1012,  1012,  1025,  1025,  1064,   999,   999,  1064,  1064,  1031,\n",
            "          1031,  1064,  1033,  1033,  1064,  1027,  1027,  1024,  1024,  1027,\n",
            "          1006,  1006,  1007,  1012,   999,  1007,  1007,  1029,  1029,  1027,\n",
            "          1009,  1009,  1006,  1024,  1007,  1010,  1010,  1006,  1051,  1051]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(summary_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "571Z7ZF9s4sd",
        "outputId": "385e52e3-ee7c-48c1-cee0-5f04440d322c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the tragedie of macbeth by william shakespeare actus primus scoena prima thunder and lightning enter three witches when shall we three meet againe in thunder lightning or in raine when the hurley burley done when the battaile lost and wonne that will be ere the set of sunne where the place vpon the heath there to meet with macbeth come gray malkin all padock calls anon faire is foule and foule is faire houer through the fogge and filthie ayre ex.. ; ; |!! | | [ [ | ] ] | = = : : = ( ( ).! ) )?? = + + ( : ),, ( o o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge\n",
        "\n",
        "from rouge import Rouge\n",
        "\n",
        "rouge = Rouge()\n",
        "\n",
        "# Generate the summary\n",
        "#input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "#summary_ids = model.generate(input_ids, num_beams=4, max_length=100, early_stopping=True)\n",
        "#summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "scores = rouge.get_scores(summary_text, text)\n",
        "print(scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fleqWN6dFO2Z",
        "outputId": "6e9229cb-b089-4394-f2b9-052df1c484bd"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.9/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from rouge) (1.16.0)\n",
            "[{'rouge-1': {'r': 1.0, 'p': 0.8108108108108109, 'f': 0.8955223831142793}, 'rouge-2': {'r': 1.0, 'p': 0.7181818181818181, 'f': 0.8359788311133507}, 'rouge-l': {'r': 1.0, 'p': 0.8108108108108109, 'f': 0.8955223831142793}}]\n"
          ]
        }
      ]
    }
  ]
}
